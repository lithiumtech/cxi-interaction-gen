{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.chdir(\"..\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# universal functions\n",
    "def cleanText (text):\n",
    "    \"\"\" \n",
    "    process: \n",
    "    - lowercase\n",
    "    - remove trailing spaces\n",
    "    - remove special characters and punction\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text_ = re.sub(r\"[^a-zA-Z0-9 ]+\",\"\", text)\n",
    "    \n",
    "    return text_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file for category mapping\n",
    "cat_file = \"./data/Rocks_N_Ropes_Chat_2022-06-21v2.csv\"\n",
    "chat_file = \"./data/RnR Chat 2021-11-11.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df = pd.read_csv(cat_file,sep=\",\")\n",
    "chat_df = pd.read_csv(chat_file,sep=\",\")\n",
    "\n",
    "display(cat_df.head())\n",
    "display(chat_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get relevant columns to rebuild the transcript \n",
    "\"\"\"\n",
    "any column with a_ or c_ will be the column with words if not 9\n",
    "\n",
    "assuming the column orders are serial in nature in building - then the transcript should be in order - tho it will not mater for the purpose of identifying the key words\n",
    "\"\"\"\n",
    "chat_cols = chat_df.columns\n",
    "agent_trans_cols = [col for col in chat_cols if \"a_\" in col]\n",
    "cust_trans_cols = [col for col in chat_cols if \"c_\" in col]\n",
    "\n",
    "# build the full text ? from the row ?  kind of slow but easier process to write\n",
    "\n",
    "def buildTranscripts(df,cols):\n",
    "    full_array = []\n",
    "    for i, row in df.iterrows():\n",
    "        text = []\n",
    "        for col in cols:\n",
    "            if row[col] !=\"9\": \n",
    "                text.append(row[col])\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        full_array.append(\" \".join(text)) # join it back\n",
    "    return full_array\n",
    "\n",
    "agent_array = buildTranscripts(chat_df,agent_trans_cols)\n",
    "customer_array =  buildTranscripts(chat_df,cust_trans_cols)\n",
    "chat_df[\"agent_transcripts\"] = agent_array # processed in serial order should align\n",
    "chat_df[\"customer_transcripts\"] = customer_array # processed in serial order should align\n",
    "chat_df.head()\n",
    "\n",
    "# save off so jeff doesn't have to reprocess this in the future\n",
    "chat_df.to_csv(\"./data/rnr_chat_w_transcript.csv\", sep=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean category df\n",
    "\n",
    "cat_df[\"Keyword_clean\"] = cat_df[\"Keyword\"].apply(lambda x: x.strip().lower()) # don't remove special as it will be indicator later\n",
    "cat_df[\"keyword_list\"] = cat_df[\"Keyword_clean\"].apply(lambda x: list(t.strip() for t in x.split(\"_\")) if \"_\" in x else [])\n",
    "\n",
    "cat_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assumptions\n",
    "\n",
    "exact match except some special characters are ignored ( . , !, etc.)  \n",
    "2. More than 1 Topic can be applied to a conversation if multiple Topics result in keyword hits. \n",
    "3. The results are not applied based on Frequency.\n",
    " 4) the scan is performed across the entire conversation (transcript) and the results are reported at the conversation level. The app does highlight all the instances of the keyword / topic hits within the conversation when reviewing the transcript\n",
    "\n",
    "\n",
    "# process flow \n",
    "- build transcript based on columns \n",
    "- for each transcript\n",
    "    - preprocess \n",
    "    - tag topic if initial key word match is found w\n",
    "\n",
    "- append transcript \n",
    "- create tag table \n",
    "    - transcript Id \n",
    "    - chat_id \n",
    "    - tag \n",
    "    (eessentially for sake of easy processing explode the df on tag array so can process in excel more easily for tying metrics back to CXI)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "cluster = LocalCluster(\"127.0.0.1:8786\", n_workers=8, threads_per_worker=8)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed transcripts\n"
     ]
    }
   ],
   "source": [
    "# build tag array for each transcript \n",
    "import re\n",
    "from sched import scheduler\n",
    "import dask.dataframe as dd\n",
    "from dask.dataframe import utils\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "category process is taking awhile so will parallelize this\n",
    "\n",
    "This as been written with dask in mind to parallelize the process across cores - embarrassingly slow and not parallel - Dask speed up 5x over 8 cores vs. pandas\n",
    "\"\"\"\n",
    "\n",
    "def wrapper(df,mapping,col):\n",
    "    l = []\n",
    "    print(df.shape)\n",
    "    df.apply(lambda x: l.append(buildTags(x.chat_number,x[col],mapping)),axis=1)\n",
    "    l = [i for i in l if not i is None]\n",
    "    if len(l)==0: \n",
    "        return pd.DataFrame(columns=[\"chat_id\",\"topic_id_array\"])\n",
    "    else:\n",
    "        df_ = pd.concat(l)\n",
    "        return df_\n",
    "\n",
    "# if this takes awhile - write this for parallel processing\n",
    "def buildTags(id,transcript,mapping_df):\n",
    "    print(id)\n",
    "    id_array = [] # collect the - will map back later\n",
    "    def topicMatch(trans, t_row):\n",
    "        if \"_\" in t_row.Keyword: \n",
    "            counter = 0\n",
    "            for token in t_row.keyword_list:\n",
    "                if token in trans.split():\n",
    "                    counter = counter + 1\n",
    "            \n",
    "            if counter >= len(t_row.Keyword):\n",
    "                id_array.append(t_row.Id)\n",
    "        else:\n",
    "            if re.search(r'\\b{}\\b'.format(t_row.Keyword),trans):\n",
    "                id_array.append(t_row.Id)\n",
    "\n",
    "        return id_array\n",
    "    \n",
    "    topic_id_array = mapping_df.apply(lambda x: topicMatch(transcript,x),axis=1) # apply across the category map\n",
    "    topic_array = list(set(itertools.chain.from_iterable(topic_id_array))) # set to consolidated list \n",
    "\n",
    "    if len(topic_array) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        id_ = [id] * len(topic_array)\n",
    "        df_ = pd.DataFrame(data=zip(id_,topic_array),columns=[\"chat_id\",\"topic_id_array\"])\n",
    "        return df_\n",
    "\n",
    "\n",
    "chat_df[\"customer_transcripts_clean\"] = chat_df.customer_transcripts.apply(lambda x: cleanText(x))\n",
    "chat_df[\"agent_transcripts_clean\"] = chat_df.agent_transcripts.apply(lambda x: cleanText(x))\n",
    "# create distributable dataframe \n",
    "chat_dd = dd.from_pandas(chat_df,npartitions=10)\n",
    "\n",
    "df_list = []\n",
    "\n",
    "df_out = chat_dd.map_partitions(lambda df: wrapper(df,cat_df,\"agent_transcripts_clean\"),meta=pd.DataFrame(columns = [\"chat_id\",\"topic_id_array\"])).compute() # add scheudler=processes if on single \n",
    "df_out.to_csv(\"./data/agent_transcript_topics.csv\", sep=\",\")\n",
    "print('completed transcripts')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutdown local distributed client on machine\n",
    "client.close()\n",
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('.dev': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed0cf586b1e59d818733435ee7616ee74ffcd2e2089d21a9a6d62947d44d89e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
