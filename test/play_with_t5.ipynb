{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\edmond.chan\\repos\\cxi-interaction-gen\\.dev\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False : ''I need to log in to my account to see if it's logged in.\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"paraphrase: I need login help\", return_tensors='pt')\n",
    "greedy_output = model.generate(input_ids, num_beams=7, no_repeat_ngram_size=2, min_length=25, max_length=75)\n",
    "message = tokenizer.decode(greedy_output[0], skip_special_tokens=True)\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "a storm in the middle of an hour. I am mad! The tent was broken during this period and it is now breaking up after about two hours, not even one day before my next job starts again? (#false)\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(f'question: {message}', return_tensors='pt')\n",
    "greedy_output = model.generate(input_ids, num_beams=1, no_repeat_ngram_size=1, min_length=50, max_length=100)\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "\n",
    "message_ = tokenizer.decode(greedy_output[0], skip_special_tokens=True)\n",
    "print(message_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Adequacy():\n",
    "  \n",
    "  def __init__(self, model_tag='prithivida/parrot_adequacy_model'):\n",
    "    from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "    self.adequacy_model = AutoModelForSequenceClassification.from_pretrained(model_tag)\n",
    "    self.tokenizer = AutoTokenizer.from_pretrained(model_tag)\n",
    "\n",
    "  def filter(self, input_phrase, para_phrases, adequacy_threshold, device=\"cpu\"):\n",
    "      top_adequacy_phrases = []\n",
    "      for para_phrase in para_phrases:\n",
    "        x = self.tokenizer(input_phrase, para_phrase, return_tensors='pt', max_length=128, truncation=True)\n",
    "        x = x.to(device)\n",
    "        self.adequacy_model = self.adequacy_model.to(device)\n",
    "        logits = self.adequacy_model(**x).logits\n",
    "        probs = logits.softmax(dim=1)\n",
    "        prob_label_is_true = probs[:,1]\n",
    "        adequacy_score = prob_label_is_true.item()\n",
    "        if adequacy_score >= adequacy_threshold:\n",
    "            top_adequacy_phrases.append(para_phrase)\n",
    "      return top_adequacy_phrases\n",
    "\n",
    "\n",
    "  def score(self, input_phrase, para_phrases, adequacy_threshold, device=\"cpu\"):\n",
    "      adequacy_scores = {}\n",
    "      for para_phrase in para_phrases:\n",
    "        x = self.tokenizer(input_phrase, para_phrase, return_tensors='pt', max_length=128, truncation=True)\n",
    "        x = x.to(device)\n",
    "        self.adequacy_model = self.adequacy_model.to(device)\n",
    "        logits = self.adequacy_model(**x).logits\n",
    "        probs = logits.softmax(dim=1)\n",
    "        prob_label_is_true = probs[:,1]\n",
    "        adequacy_score = prob_label_is_true.item()\n",
    "        if adequacy_score >= adequacy_threshold:\n",
    "          adequacy_scores[para_phrase] = adequacy_score\n",
    "      return adequacy_scores\n",
    "\n",
    "class Fluency():\n",
    "  def __init__(self, model_tag='prithivida/parrot_fluency_model'):\n",
    "    from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "    self.fluency_model = AutoModelForSequenceClassification.from_pretrained(model_tag, num_labels=2)\n",
    "    self.fluency_tokenizer = AutoTokenizer.from_pretrained(model_tag)\n",
    "\n",
    "  def filter(self, para_phrases, fluency_threshold, device=\"cpu\"):\n",
    "      import numpy as np\n",
    "      from scipy.special import softmax\n",
    "      self.fluency_model = self.fluency_model.to(device)\n",
    "      top_fluent_phrases = []\n",
    "      for para_phrase in para_phrases:\n",
    "        input_ids = self.fluency_tokenizer(\"Sentence: \" + para_phrase, return_tensors='pt', truncation=True)\n",
    "        input_ids = input_ids.to(device)\n",
    "        prediction = self.fluency_model(**input_ids)\n",
    "        scores = prediction[0][0].detach().cpu().numpy()\n",
    "        scores = softmax(scores)\n",
    "        fluency_score = scores[1] # LABEL_0 = Bad Fluency, LABEL_1 = Good Fluency\n",
    "        if fluency_score >= fluency_threshold:\n",
    "          top_fluent_phrases.append(para_phrase)\n",
    "      return top_fluent_phrases\n",
    "\n",
    "  def score(self, para_phrases, fluency_threshold, device=\"cpu\"):\n",
    "      import numpy as np\n",
    "      from scipy.special import softmax\n",
    "      self.fluency_model = self.fluency_model.to(device)\n",
    "      fluency_scores = {}\n",
    "      for para_phrase in para_phrases:\n",
    "        input_ids = self.fluency_tokenizer(\"Sentence: \" + para_phrase, return_tensors='pt', truncation=True)\n",
    "        input_ids = input_ids.to(device)\n",
    "        prediction = self.fluency_model(**input_ids)\n",
    "        scores = prediction[0][0].detach().cpu().numpy()\n",
    "        scores = softmax(scores)\n",
    "        fluency_score = scores[1] # LABEL_0 = Bad Fluency, LABEL_1 = Good Fluency\n",
    "        if fluency_score >= fluency_threshold:\n",
    "          fluency_scores[para_phrase] = fluency_score\n",
    "      return fluency_scores\n",
    "      \n",
    "\n",
    "\n",
    "class Diversity():\n",
    "\n",
    "  def __init__(self, model_tag='paraphrase-distilroberta-base-v2'):\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    self.diversity_model = SentenceTransformer(model_tag)\n",
    "\n",
    "  def rank(self, input_phrase, para_phrases, diversity_ranker='levenshtein'):\n",
    "      if diversity_ranker == \"levenshtein\":\n",
    "        return self.levenshtein_ranker(input_phrase, para_phrases)\n",
    "      elif diversity_ranker == \"euclidean\":\n",
    "        return self.euclidean_ranker(input_phrase, para_phrases)\n",
    "      elif diversity_ranker == \"diff\":\n",
    "        return self.diff_ranker(input_phrase, para_phrases)\n",
    "\n",
    "  def euclidean_ranker(self, input_phrase, para_phrases):\n",
    "      import pandas as pd\n",
    "      from sklearn_pandas import DataFrameMapper\n",
    "      from sklearn.preprocessing import MinMaxScaler\n",
    "      from scipy import spatial\n",
    "\n",
    "      diversity_scores = {}\n",
    "      outputs = []\n",
    "      input_enc = self.diversity_model.encode(input_phrase.lower())\n",
    "      for para_phrase in para_phrases:              \n",
    "          paraphrase_enc = self.diversity_model.encode(para_phrase.lower())\n",
    "          euclidean_distance = (spatial.distance.euclidean(input_enc, paraphrase_enc))\n",
    "          outputs.append((para_phrase,  euclidean_distance))\n",
    "      df = pd.DataFrame(outputs, columns=['paraphrase', 'scores'])\n",
    "      fields = []\n",
    "      for col in df.columns:\n",
    "          if col == \"scores\":\n",
    "              tup = ([col], MinMaxScaler())\n",
    "          else:  \n",
    "              tup = ([col], None)\n",
    "          fields.append(tup) \n",
    "\n",
    "      mapper = DataFrameMapper(fields, df_out=True)\n",
    "      for index, row in mapper.fit_transform(df.copy()).iterrows():\n",
    "          diversity_scores[row['paraphrase']] = row['scores']\n",
    "      return  diversity_scores\n",
    "\n",
    "  def levenshtein_ranker(self, input_phrase, para_phrases):\n",
    "      import Levenshtein\n",
    "      diversity_scores = {}\n",
    "      for para_phrase in para_phrases:              \n",
    "          distance = Levenshtein.distance(input_phrase.lower(), para_phrase)\n",
    "          diversity_scores[para_phrase] =  distance\n",
    "      return diversity_scores\n",
    "  \n",
    "  def diff_ranker(self, input_phrase, para_phrases):\n",
    "    import difflib\n",
    "    differ = difflib.Differ()\n",
    "    diversity_scores ={}\n",
    "    for para_phrase in para_phrases:\n",
    "        diff = differ.compare(input_phrase.split(), para_phrase.split())\n",
    "        count = 0\n",
    "        for d in diff:\n",
    "          if \"+\" in d or \"-\" in d:\n",
    "            count += 1\n",
    "        diversity_scores[para_phrase] = count\n",
    "    return diversity_scores\n",
    "\n",
    "\n",
    "def reprhase(input_phrase, tokenizer,model,use_gpu=False, diversity_ranker=\"levenshtein\", do_diverse=False, style=1, max_length=32, adequacy_threshold = 0.90, fluency_threshold = 0.90): \n",
    "    if use_gpu:\n",
    "        device= \"cuda:0\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    import re\n",
    "    save_phrase = input_phrase\n",
    "    if len(input_phrase) >= max_length:\n",
    "        max_length += 32 \t\n",
    "    input_phrase = re.sub('[^a-zA-Z0-9 \\?\\'\\-\\/\\:\\.]', '', input_phrase)\n",
    "    input_phrase = \"paraphrase: \" + input_phrase\n",
    "    print(input_phrase)\n",
    "    input_ids = tokenizer.encode(input_phrase, return_tensors='pt')\n",
    "    input_ids = input_ids.to(device)\n",
    "    max_return_phrases = 10\n",
    "\n",
    "    # for n in range(2, 9):\n",
    "    #     if max_return_phrases % n == 0:\n",
    "    #         break \n",
    "    #print(\"max_return_phrases - \", max_return_phrases , \" and beam groups -\", n)            \n",
    "    preds = model.generate(\n",
    "                input_ids,\n",
    "                do_sample=True, \n",
    "                max_length=max_length, \n",
    "                top_k=50, \n",
    "                top_p=0.95, \n",
    "                early_stopping=True,\n",
    "                num_return_sequences=max_return_phrases)\n",
    "\n",
    "\n",
    "    paraphrases= set()\n",
    "\n",
    "    for pred in preds:\n",
    "        gen_pp = tokenizer.decode(pred, skip_special_tokens=True).lower()\n",
    "        gen_pp = re.sub('[^a-zA-Z0-9 \\?\\'\\-]', '', gen_pp)\n",
    "        paraphrases.add(gen_pp)\n",
    "\n",
    "    adq = Adequacy()\n",
    "    flu = Fluency()\n",
    "    div = Diversity()\n",
    "    adequacy_filtered_phrases = adq.filter(input_phrase, paraphrases, adequacy_threshold, device )\n",
    "    if len(adequacy_filtered_phrases) > 0 :\n",
    "      fluency_filtered_phrases = flu.filter(adequacy_filtered_phrases, fluency_threshold, device )\n",
    "      if len(fluency_filtered_phrases) > 0 :\n",
    "          diversity_scored_phrases = div.rank(input_phrase, fluency_filtered_phrases, diversity_ranker)\n",
    "          para_phrases = []\n",
    "          for para_phrase, diversity_score in diversity_scored_phrases.items():\n",
    "              para_phrases.append((para_phrase, diversity_score))\n",
    "          para_phrases.sort(key=lambda x:x[1], reverse=True)\n",
    "          return para_phrases[0]\n",
    "      else:\n",
    "          return [(save_phrase,0)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paraphrase: Can you help me login to my account?\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "false\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n"
     ]
    }
   ],
   "source": [
    "\n",
    "blah = reprhase(input_phrase=\"Can you help me login to my account?\", tokenizer=tokenizer, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'contradiction',\n",
       " 'falsch',\n",
       " 'falses',\n",
       " 'negative',\n",
       " 'neutral',\n",
       " 'positive',\n",
       " 'true'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('.dev': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed0cf586b1e59d818733435ee7616ee74ffcd2e2089d21a9a6d62947d44d89e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
